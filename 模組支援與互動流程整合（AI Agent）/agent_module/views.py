# === ğŸ“¦ imports ===

# æ¨™æº–åº«
import json
import re
import random

# Django
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt

# DRF
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.permissions import AllowAny
from rest_framework import status
from rest_framework.test import APIRequestFactory  # âœ… æ–°å¢é€™è¡Œ
from rest_framework.request import Request
from rest_framework.parsers import JSONParser

# æœ¬åœ° utilsï¼ˆå…±ç”¨é‚è¼¯ï¼‰
from .utils_card import (
    generate_map_url,
    format_open_status,
    extract_district,
    generate_price_description,
    generate_recommend_reason
)


# åŠŸèƒ½ 1ï¼šåå‘æ¨è–¦æ¢ä»¶æ“·å–ï¼ˆå¼·åŒ–ç‰ˆ v6 - èªæ„è£œå¼·å®Œå…¨å‘½ä¸­ï¼‰
class ExtractNegativeConditionsView(APIView):
    permission_classes = [AllowAny]

    def post(self, request):
        req_type = request.data.get('type')
        user_input = request.data.get('text', '').strip()

        if req_type != 'text' or not user_input:
            return Response({
                "status": "error",
                "data": None,
                "message": "è«‹æä¾› type='text' ä¸”åŒ…å« text æ¬„ä½"
            }, status=status.HTTP_400_BAD_REQUEST)

        # --- åŸºæœ¬è¨­å®š ---
        prefix = r'(?:æˆ‘|ä¸é|é‚£å°±|å¯èƒ½)?'
        negative_verbs = r'(ä¸æƒ³åƒ|ä¸æƒ³è¦|ä¸è¦|ä¸åƒ|åˆ¥æ¨è–¦|ä¸è¦æ¨è–¦|ä¸å¤ªæƒ³åƒ|æ²’æœ‰å¾ˆå–œæ­¡|é‚£ç¨®æˆ‘ä¸æ„›|ä¸æœƒé¸|ä¸å¤ªå–œæ­¡|ä¸å–œæ­¡|ä¸æ„›|æˆ‘ä¸æœƒé¸|ä¸è€ƒæ…®|ç„¡æ³•æ¥å—)'
        pattern = rf'{prefix}{negative_verbs}(.+?)(?:[ï¼Œã€‚!ï¼,\.\s]|$)'
        matches = re.findall(pattern, user_input)

        FUNCTION_PREFIXES = ['æ¨è–¦', 'é¤å»³', 'åœ°æ–¹', 'é‚£å®¶', 'é€™å®¶', 'åº—å®¶', 'åƒ', 'æƒ³åƒ', 'æä¾›']
        TAIL_PARTICLES = r'[çš„äº†å‘¢å•¦å•Šå˜›å”·å–”å“¦è€¶å‘€å›‰å§]*$'
        PRESERVE_TERMS = ['åƒåˆ°é£½', 'æ—©åˆé¤', 'å®µå¤œ', 'å¥—é¤', 'å…§ç”¨', 'å¤–å¸¶']
        CLEAN_SUFFIXES = ['çš„æ–™ç†', 'æ–™ç†', 'åº—å®¶', 'é¤å»³', 'é¡å‹', 'é¡', 'é‚£å®¶', 'é€™å®¶', 'åº—']
        EXCLUSION_WHITELIST = ['è¾£å¦¹', 'è¾£å€‹', 'è¾£å€‹å¥³ç”Ÿ', 'ç«è¾£çš„éŸ³æ¨‚']
        BLACKLIST_SUFFIX = ['é‚£ç¨®', 'é€™ç¨®']

        SEMANTIC_NEGATIVE_MAP = {
            "å¤ªæ²¹": "æ²¹è†©",
            "å¾ˆæ²¹": "æ²¹è†©",
            "æ²¹è†©": "æ²¹è†©",
            "å¤ªè†©": "æ²¹è†©",
            "åƒå®Œæœƒè†©": "æ²¹è†©",
            "ç”œåˆ°è†©": "ç”œè†©",
            "å¤ªè²´": "é«˜åƒ¹",
            "åƒ¹æ ¼å¤ªé«˜": "é«˜åƒ¹",
            "CP å€¼å¤ªä½": "é«˜åƒ¹",
            "ä»½é‡å°‘åˆè²´": "é«˜åƒ¹",
            "ä¸å¤ é£½": "ä»½é‡å°‘",
            "å¤ªåµ": "åµé›œ",
            "å¾ˆåµ": "åµé›œ",
            "å¤ªæ“ ": "æ“æ“ ",
            "ä¸å¤ªä¹¾æ·¨": "ä¸ä¹¾æ·¨",
            "è¡›ç”Ÿä¸å¥½": "ä¸ä¹¾æ·¨",
            "ä¸ä¹¾æ·¨": "ä¸ä¹¾æ·¨",
            "å¤ªé¹¹": "é‡å£å‘³",
            "å¤ªè¾£": "é‡å£å‘³",
            "å¤ªé¹¹å¤ªè¾£": "é‡å£å‘³",
            "å¤ªå¤šé†¬": "é†¬å¤š",
            "å¤ªå¤šé†¬çš„": "é†¬å¤š",
            "é›·": "é›·åº—",
            "æœ‰é»é›·": "é›·åº—",
            "é›·åº—": "é›·åº—",
            "å¤ªæ–‡é’": "æ–‡é’é¢¨æ ¼",
            "é€™ç¨®å¤ªæ–‡é’çš„": "æ–‡é’é¢¨æ ¼",
            "ç¶²ç¾åº—": "ç¶²ç¾åº—",
            "æ‰“å¡åº—": "ç¶²ç¾åº—",
            "Instagram æ‰“å¡": "ç¶²ç¾åº—"
        }

        excluded_items = []

        for match in matches:
            phrase = match[1] if isinstance(match, tuple) and len(match) > 1 else match[0] if isinstance(match, tuple) else match
            split_words = re.split(r'[,ã€ï¼Œå’Œè·Ÿä»¥åŠæˆ–é‚„æœ‰\s]+', phrase)

            for word in split_words:
                word = word.strip()
                if not word or word in EXCLUSION_WHITELIST or word in BLACKLIST_SUFFIX:
                    continue

                if word in PRESERVE_TERMS:
                    cleaned = word
                elif word.endswith("çš„") and word[:-1] in PRESERVE_TERMS:
                    cleaned = word[:-1]
                else:
                    for prefix_word in FUNCTION_PREFIXES:
                        if word.startswith(prefix_word):
                            word = word[len(prefix_word):]
                            break
                    word = re.sub(TAIL_PARTICLES, '', word)
                    for suffix in CLEAN_SUFFIXES:
                        if word.endswith(suffix) and len(word) > len(suffix):
                            word = word[:-len(suffix)]
                            break
                    cleaned = word

                if cleaned and len(cleaned) <= 6 and re.match(r'^[\u4e00-\u9fffA-Za-z0-9]+$', cleaned):
                    excluded_items.append(cleaned)

        for phrase, keyword in SEMANTIC_NEGATIVE_MAP.items():
            if phrase in user_input and keyword not in excluded_items:
                excluded_items.append(keyword)

        final_excluded = []
        for kw in excluded_items:
            normalized = SEMANTIC_NEGATIVE_MAP.get(kw, kw)
            if normalized not in final_excluded:
                final_excluded.append(normalized)

        return Response({
            "status": "success",
            "data": {
                "excluded": final_excluded
            },
            "message": "å·²æ“·å–åå‘æ¨è–¦æ¢ä»¶"
        }, status=status.HTTP_200_OK)


# åŠŸèƒ½ 2ï¼šæ¨è–¦ç†ç”±è£œå¼· + çµæ§‹åŒ–è¼¸å‡ºï¼ˆå¼·åŒ–ç‰ˆï¼šå»é‡èˆ‡æ’åº + ä¿åº•æ’é™¤é–˜é–€ï¼‰
class GenerateRecommendReasonView(APIView):
    permission_classes = [AllowAny]

    def post(self, request):
        from collections import OrderedDict
        import re, json

        DEBUG = True  # æƒ³é—œé–‰çµ‚ç«¯ debug å°å‡ºå°±è¨­ç‚º False

        # âœ… æ¥æ”¶ user_inputï¼ˆå¯é¸ï¼‰èˆ‡ä¸Šæ¸¸å‚³å…¥çš„ excluded_itemsï¼ˆå¯é¸ï¼‰
        user_input = (request.data.get("user_input", "") if hasattr(request, "data") else "").lower().strip()
        req_excluded_items = (request.data.get("excluded_items", []) if hasattr(request, "data") else []) or []

        if hasattr(request, 'data'):
            req_type = request.data.get('type')
            restaurants = request.data.get('restaurants', [])
        else:
            req_type = request.POST.get('type')
            try:
                restaurants = json.loads(request.body.decode()).get("restaurants", [])
            except Exception:
                restaurants = []

        if req_type != 'restaurant_list' or not isinstance(restaurants, list):
            return Response({
                "status": "error",
                "data": None,
                "message": "è«‹æä¾› type='restaurant_list' ä¸”åŒ…å« restaurants æ¸…å–®"
            }, status=status.HTTP_400_BAD_REQUEST)

        # ======================
        # ğŸ›¡ï¸ ä¿åº•æ’é™¤é–˜é–€ï¼ˆé—œéµä¿®æ­£ï¼‰
        def _normalize(s: str) -> str:
            if not isinstance(s, str):
                return ""
            s = s.strip().lower()
            s = re.sub(r"[ï¼Œ,ã€‚.!ï¼Ÿ?ã€/\\|()\[\]ã€ã€‘{}\-ï¼¿_~^'\"`ï¼š:ï¼›;@#*$ï¼‹+ï¼=ï¼Â·ï½¥\s]+", "", s)
            return s

        def _extract_negatives(text: str) -> list:
            if not text:
                return []
            # 1) æŠŠå¦å®šè©æ“·å–å¼å­æ”¹æˆï¼šåˆ†éš”ç¬¦å¾Œé¢ä¸€å®šè¦è·Ÿè©
            neg_pat = re.compile(
                r"(?:ä¸æƒ³åƒ|ä¸æƒ³è¦|ä¸è¦|ä¸åƒ|åˆ¥æ¨è–¦|ä¸è¦æ¨è–¦)\s*"
                r"([^\nï¼Œ,ã€‚ï¼!ï¼Ÿ?\s]+(?:(?:[ã€,/å’Œèˆ‡åŠæˆ–]|æˆ–æ˜¯|[,ï¼Œ/ ])+[^\nï¼Œ,ã€‚ï¼!ï¼Ÿ?\s]+)*)"
            )
            m = neg_pat.search(text)
            if not m:
                return []
            seg = m.group(1)
            # 2) åˆ†è©å™¨ç¶­æŒä½ æ–°åŠ çš„ç‰ˆæœ¬å³å¯ï¼ˆOKï¼‰
            parts = re.split(r"(?:[ã€,/å’Œèˆ‡åŠæˆ–]|æˆ–æ˜¯|[,ï¼Œ\s])+", seg)
            return [p.strip() for p in parts if p.strip()]

        def _expand_exclusions(items: list) -> list:
            mapping = {
                "ç”œé»": ["ç”œé»", "ç”œå“", "ç”œé£Ÿ", "è›‹ç³•", "çƒ˜ç„™", "ä¸‹åˆèŒ¶", "ç”œé»åº—", "ç”œé»å°ˆé–€", "ç”œé»è©•åƒ¹é«˜"],
                "æ‹‰éºµ": ["æ‹‰éºµ", "ramen"],
                "ç‡’çƒ¤": ["ç‡’çƒ¤", "çƒ¤è‚‰", "ç‚­ç«", "ç‡’è‚‰"],
                "æ¼¢å ¡": ["æ¼¢å ¡", "burger"],
                "ç¾å¼": ["ç¾å¼", "ç¾å¼é¤å»³", "ç¾å¼é¢¨æ ¼", "ç¾å¼æ¼¢å ¡"],
                "ç«é‹": ["ç«é‹", "é‹ç‰©", "æ¶®æ¶®é‹", "éº»è¾£é‹"],
            }
            expanded = set()
            for it in items or []:
                raw = (it or "").strip()
                norm = _normalize(raw)
                if not norm:
                    continue
                candidates = set([raw, norm])
                for key, syns in mapping.items():
                    if key == raw or key == norm:
                        candidates |= set(syns)
                for c in candidates:
                    expanded.add(_normalize(c))
            return list(expanded)

        def _collect_blob(r: dict) -> str:
            parts = []
            for k in ["name", "highlight", "ai_reason", "comment_summary", "style", "address"]:
                v = r.get(k, "")
                if v:
                    parts.append(str(v))
            for lk in ["matched_tags", "tags", "features"]:
                seq = r.get(lk) or []
                parts.extend([str(t) for t in seq])
            return _normalize("ï½œ".join(parts))

        extracted_from_input = _extract_negatives(user_input)
        raw_excluded = []
        for x in req_excluded_items + extracted_from_input:
            if x and x not in raw_excluded:
                raw_excluded.append(x)

        expanded_ex = set(_expand_exclusions(raw_excluded))
        if DEBUG: print("ğŸ§©(åŠŸèƒ½äºŒ) æ“´å……å¾Œæ’é™¤è©ï¼š", sorted(expanded_ex))

        filtered_restaurants = []
        debug_hits = []
        for r in restaurants:
            blob = _collect_blob(r)
            hit = None
            for ex in expanded_ex:
                if ex and ex in blob:
                    hit = ex
                    break
            if hit:
                debug_hits.append((r.get("name", ""), hit))
                continue
            filtered_restaurants.append(r)

        if len(filtered_restaurants) == len(restaurants) and raw_excluded:
            tmp = []
            for r in filtered_restaurants:
                name = r.get("name", "")
                if any(x in name for x in raw_excluded):
                    debug_hits.append((name, f"fallback:{'/'.join(raw_excluded)}"))
                else:
                    tmp.append(r)
            filtered_restaurants = tmp

        if DEBUG: print("ğŸ”(åŠŸèƒ½äºŒ) è¢«æ’é™¤æ¸…å–®/å‘½ä¸­è©ï¼š", debug_hits)

        restaurants = filtered_restaurants
        # ======================

        # âœ… èªæ„è£œå¼·è¦å‰‡ï¼ˆåŸæ¨£ä¿ç•™ï¼‰
        user_input_rules = {
            "åƒç´ ": "ç´ é£Ÿéœ€æ±‚", "ç´ é£Ÿ": "ç´ é£Ÿéœ€æ±‚",
            "æ€•è¾£": "é¿å…è¾›è¾£æ–™ç†", "ä¸åƒè¾£": "é¿å…è¾›è¾£æ–™ç†",
            "ä¸æƒ³å¤ªæ²¹": "æ¸…çˆ½å£å‘³", "æ¸…çˆ½": "æ¸…çˆ½å£å‘³", "å¤ªæ²¹": "æ¸…çˆ½å£å‘³", "æ²¹è†©": "æ¸…çˆ½å£å‘³",
            "æœ‹å‹èšé¤": "é©åˆæœ‹å‹èšæœƒ", "åŒå­¸èšé¤": "é©åˆæœ‹å‹èšæœƒ", "èšé¤": "é©åˆèšé¤",
            "å®¶åº­èšé¤": "é©åˆå®¶åº­èšæœƒ", "å¸¶çˆ¸åª½": "é©åˆå®¶åº­èšæœƒ", "çˆ¸åª½": "é©åˆå®¶åº­èšæœƒ", "å®¶äººåƒé£¯": "é©åˆå®¶åº­èšæœƒ",
            "ç´„æœƒ": "æ°£æ°›ä½³ï¼Œé©åˆç´„æœƒ", "å•†å‹™": "é©åˆæ­£å¼èšæœƒ", "è«‹å®¢": "é©åˆæ­£å¼èšæœƒ", "æ­£å¼": "é©åˆæ­£å¼èšæœƒ",
            "æ…¶ç”Ÿ": "é©åˆæ…¶ç¥å ´åˆ", "ç”Ÿæ—¥": "é©åˆæ…¶ç¥å ´åˆ", "æ…¶ç¥": "é©åˆæ…¶ç¥å ´åˆ",
            "å°å­©": "è¦ªå­å‹å–„", "å…’ç«¥": "è¦ªå­å‹å–„",
            "ä¸è²´": "åƒ¹æ ¼å¯¦æƒ ", "ä¾¿å®œ": "åƒ¹æ ¼å¯¦æƒ ", "å¹³åƒ¹": "åƒ¹æ ¼å¯¦æƒ ", "åƒ¹æ ¼å¯¦æƒ ": "åƒ¹æ ¼å¯¦æƒ ",
            "é«˜ç´š": "ç²¾ç·»é«˜åƒ¹", "é«˜åƒ¹": "ç²¾ç·»é«˜åƒ¹", "é«˜ç«¯": "ç²¾ç·»é«˜åƒ¹", "ç²¾ç·»": "ç²¾ç·»é«˜åƒ¹",
            "å®µå¤œ": "é©åˆå®µå¤œ", "æ·±å¤œ": "é©åˆå®µå¤œ", "æ—©åˆé¤": "é©åˆæ—©åˆé¤", "æ—©é¤": "é©åˆæ—©é¤",
            "æ™‚é–“ä¸å¤š": "å¿«é€Ÿæ–¹ä¾¿", "è¶•æ™‚é–“": "å¿«é€Ÿæ–¹ä¾¿", "å¿«é€Ÿåƒ": "å¿«é€Ÿæ–¹ä¾¿",
            "æƒ³åƒè¾£": "é‡å£å‘³æ–™ç†", "é‡å£å‘³": "é‡å£å‘³æ–™ç†", "è¾£çš„æ–™ç†": "é‡å£å‘³æ–™ç†", "éº»è¾£": "é‡å£å‘³æ–™ç†", "è¾£é‹": "é‡å£å‘³æ–™ç†",
        }

        # âœ… ä¿åºå»é‡ï¼ˆç”¨åœ¨ tagsï¼‰
        def uniq_keep_order(items):
            seen = set()
            out = []
            for it in items:
                if it not in seen:
                    seen.add(it)
                    out.append(it)
            return out

        # âœ… èªæ„å»é‡ + æ’åºï¼ˆåŸæ¨£ä¿ç•™ï¼‰
        def deduplicate_semantic(phrases):
            cleaned = []
            seen = set()
            phrases_sorted = sorted(phrases, key=lambda x: -len(x))
            for p in phrases_sorted:
                if all(p not in s for s in seen):
                    cleaned.append(p)
                    seen.add(p)
            return list(OrderedDict.fromkeys(cleaned))

        def sort_reasons(reason_list):
            priority = [
                "ç´ é£Ÿ", "è¾›è¾£", "æ¸…çˆ½", "é‡å£å‘³",
                "è©•åƒ¹", "ç†±é–€", "æ°£æ°›",
                "åƒ¹æ ¼", "CP", "é«˜åƒ¹", "ä¾¿å®œ",
                "åœ°é»", "ä½æ–¼",
                "èšé¤", "ç´„æœƒ", "å®¶åº­", "å®µå¤œ", "æ—©é¤", "æ…¶ç¥", "è¦ªå­",
                "é¢¨æ ¼", "ç‡Ÿæ¥­", "å¤œè²“"
            ]
            return sorted(reason_list, key=lambda r: next((i for i, p in enumerate(priority) if p in r), len(priority)))

        results = []

        for restaurant in restaurants:
            name = restaurant.get('name', '')
            rating = restaurant.get('rating', 0)
            address = restaurant.get('address', '')
            is_open_raw = restaurant.get('is_open', None)
            ai_reason = restaurant.get('ai_reason', '')
            comment_summary = restaurant.get('comment_summary', '')
            highlight = restaurant.get('highlight', '')
            matched_tags = restaurant.get('matched_tags', [])
            distance_m = restaurant.get('distance_m', None)
            distance = f"{distance_m} å…¬å°º" if distance_m else "æœªçŸ¥"
            reason_score = restaurant.get('reason_score', 0)
            price_level = restaurant.get('price_level', '')
            review_count = restaurant.get('review_count', None)

            map_url = generate_map_url(name)
            is_open = format_open_status(is_open_raw)
            price_desc = generate_price_description(price_level)
            district = extract_district(address)

            reason_source = "inference"
            if ai_reason:
                core_reason = ai_reason
                reason_source = "ai"
            elif comment_summary:
                core_reason = comment_summary
                reason_source = "summary"
            else:
                core_reasons = []
                if rating >= 4.5:
                    core_reasons.append("è©•åƒ¹å¾ˆé«˜")
                if "å°åŒ—" in address or "æ–°åŒ—" in address:
                    core_reasons.append("åœ°é»æ–¹ä¾¿")
                if not core_reasons:
                    core_reasons.append("æ•´é«”è©•åƒ¹ä¸éŒ¯")
                core_reason = "ã€".join(core_reasons)

            extra_reasons = []
            if highlight:
                extra_reasons.append(highlight)
            if matched_tags:
                extra_reasons.extend(matched_tags)
            if price_desc:
                extra_reasons.append(price_desc)
            if district:
                extra_reasons.append(f"ä½æ–¼{district}")

            features = restaurant.get("features", [])
            style = restaurant.get("style", "")
            opening_hours = restaurant.get("opening_hours", "")

            feature_map = {
                "ç”œé»å°ˆé–€": "ç”œé»è©•åƒ¹é«˜",
                "æ°£æ°›ä½³": "æ°£æ°›ä½³",
                "èšé¤æ¨è–¦": "é©åˆèšé¤",
                "é«˜ CP å€¼": "é«˜ CP å€¼",
                "åƒ¹æ ¼ä¾¿å®œ": "åƒ¹æ ¼å¯¦æƒ ",
                "åƒ¹æ ¼è¦ªæ°‘": "åƒ¹æ ¼å¯¦æƒ ",
                "äººæ°£é¤å»³": "ç†±é–€åº—å®¶",
                "å®µå¤œå¥½é¸æ“‡": "é©åˆå®µå¤œ",
                "ç•°åœ‹æ–™ç†": "ç•°åœ‹é¢¨å‘³"
            }
            for f in features:
                if f in feature_map:
                    extra_reasons.append(feature_map[f])

            style_map = {
                "æ–‡é’": "æ–‡é’é¢¨æ ¼",
                "ç¾å¼": "ç¾å¼é¢¨æ ¼",
                "æ—¥å¼": "æ—¥å¼é¢¨æ ¼",
                "å¤œè²“æ—": "é©åˆå¤œè²“å­",
                "æ±å—äºé¢¨": "æ±å—äºé¢¨æ ¼"
            }
            if style in style_map:
                extra_reasons.append(style_map[style])

            if opening_hours:
                if "00" in opening_hours or "02" in opening_hours:
                    extra_reasons.append("å¤œé–“ç‡Ÿæ¥­")
                if "23" in opening_hours or "22" in opening_hours:
                    extra_reasons.append("é©åˆå®µå¤œ")
                if "å…¨å¤©" in opening_hours:
                    extra_reasons.append("å…¨å¤©ç‡Ÿæ¥­")

            if user_input:
                for keyword, reason in user_input_rules.items():
                    if keyword in user_input:
                        extra_reasons.append(reason)

            # âœ… èªæ„å»é‡ + åˆ†é¡æ’åº
            extra_reasons_cleaned = deduplicate_semantic(extra_reasons)
            extra_reasons_sorted = sort_reasons(extra_reasons_cleaned)

            reason_summary = {
                "source": reason_source,
                "core": core_reason,
                "extra": extra_reasons_sorted
            }
            full_reason = "ã€".join([core_reason] + extra_reasons_sorted)

            # âœ… tagsï¼šä¿åºå»é‡ï¼ˆé¿å… set æ‰“äº‚é †åºï¼‰
            combined_tags = uniq_keep_order(list(matched_tags) + list(extra_reasons_sorted))

            results.append({
                "name": name,
                "address": address,
                "rating": rating,
                "price_level": price_level,
                "review_count": review_count,
                "highlight": highlight,
                "tags": combined_tags,
                "matched_tags": matched_tags,
                "is_open": is_open,
                "distance": distance,
                "reason_score": reason_score,
                "map_url": map_url,
                "reason_summary": reason_summary,
                "recommend_reason": full_reason
            })

        sorted_results = sorted(results, key=lambda x: (
            x.get('reason_score') or 0,
            x.get('rating') or 0,
            x.get('review_count') or 0
        ), reverse=True)

        return Response({
            "status": "success",
            "data": {
                "results": sorted_results
            },
            "message": "æ¨è–¦ç†ç”±å·²ç”¢ç”Ÿ"
        }, status=status.HTTP_200_OK)
    

# åŠŸèƒ½ 3-1ï¼šæ¨¡ç³Šèªå¥æç¤ºï¼ˆæœ€çµ‚å„ªåŒ–ç‰ˆï¼‰
class GeneratePromptView(APIView):
    permission_classes = [AllowAny]

    def post(self, request):
        req_type = request.data.get("type")
        user_input = request.data.get("text", "").strip()

        if req_type != "text" or not user_input:
            return Response({
                "status": "error",
                "data": None,
                "message": "è«‹æä¾› type='text' ä¸”åŒ…å« text æ¬„ä½"
            }, status=status.HTTP_400_BAD_REQUEST)

        # æ¨¡ç³Šèªå¥ä¾ç…§ç¨‹åº¦åˆ†é¡ï¼ˆå¯æ“´å……ï¼‰
        vague_patterns = {
            "vague": [
                "éš¨ä¾¿", "ä½ æ±ºå®š", "ä¸çŸ¥é“", "ä¸æ¸…æ¥š", "æ²’æ„è¦‹", "æ²’æƒ³åƒçš„", "ä¸çŸ¥é“åƒä»€éº¼", "ä¸ç¢ºå®š", "æ²’éˆæ„Ÿ", "éš¨ä½ "
            ],
            "medium": [
                "éƒ½å¯ä»¥", "ç„¡æ‰€è¬‚", "ä½ çœ‹è‘—è¾¦", "ä½ å¹«æˆ‘é¸", "å†èªªå§", "çœ‹å¿ƒæƒ…", "çœ‹è‘—è¾¦", "å¯ä»¥å•Šéƒ½è¡Œ", "æ²’é—œä¿‚"
            ],
            "slight": [
                "æ²’æƒ³æ³•", "é‚„æ²’æƒ³å¥½", "æ²’ç‰¹åˆ¥æƒ³åƒ", "é‚„ä¸çŸ¥é“åƒä»€éº¼", "éœ€è¦æƒ³ä¸€ä¸‹", "å†çœ‹çœ‹", "å†æƒ³æƒ³"
            ]
        }


        level = "clear"
        guidance = "æ­¡è¿å‘Šè¨´æˆ‘å€‘ä»Šå¤©æƒ³åƒä»€éº¼ï¼Œæˆ–ä¹Ÿå¯ä»¥æä¾›ä¸æƒ³åƒçš„é¡å‹ï¼Œæˆ‘å€‘æœƒå¹«ä½ æŒ‘é¸é©åˆçš„é¤å»³ï¼"

        # éæ­·æ‰€æœ‰æ¨¡ç³Šç­‰ç´šï¼Œä¾åºæ¯”å°
        for current_level, keywords in vague_patterns.items():
            if any(keyword in user_input for keyword in keywords):
                level = current_level
                if level == "slight":
                    guidance = "ä»Šå¤©æƒ³åƒé»ç°¡å–®çš„é‚„æ˜¯ä¾†é»ç‰¹åˆ¥çš„å‘¢ï¼Ÿå¹¾å€‹æ–¹å‘å¹«ä½ ç™¼æƒ³ä¸€ä¸‹ï½"
                elif level == "medium":
                    guidance = "é‚£ä½ åå¥½ä»€éº¼é¡å‹ï¼Ÿæˆ–æœ‰ä¸å–œæ­¡çš„æ–™ç†å—ï¼Ÿæˆ‘å€‘å¯ä»¥å¹«ä½ æ’é™¤ä¸€éƒ¨åˆ†å–”ï¼"
                elif level == "vague":
                    guidance = "å¯ä»¥å…ˆå¾ã€ä¸æƒ³åƒä»€éº¼ã€é–‹å§‹è¬›èµ·å”·ï½åƒæ˜¯ä¸åƒè¾£ã€ä¸åƒç‚¸ç‰©ä¹‹é¡çš„éƒ½å¯ä»¥èªªå‡ºä¾†ï¼"
                break

        return Response({
            "status": "success",
            "data": {
                "level": level,
                "guidance": guidance
            },
            "message": "æ¨¡ç³Šèªå¥æç¤ºå·²ç”¢ç”Ÿ"
        }, status=status.HTTP_200_OK)


# åŠŸèƒ½ 3-2ï¼šäº’å‹•å¼èªå¥å¼•å°å»ºè­°ï¼ˆæœ€çµ‚å¼·åŒ–ç‰ˆ2ï¼‰

class SuggestInputGuidanceView(APIView):
    permission_classes = [AllowAny]

    def post(self, request):
        req_type = request.data.get("type")
        user_input = request.data.get("text", "").lower().strip()

        if req_type != "text" or not user_input:
            return Response({
                "status": "error",
                "data": None,
                "message": "è«‹æä¾› type='text' ä¸”åŒ…å« text æ¬„ä½"
            }, status=status.HTTP_400_BAD_REQUEST)

        summary = []
        default_guidance = "æ‚¨å¯ä»¥è¼¸å…¥æƒ³åƒçš„é¡å‹ã€å ´åˆã€é ç®—ç­‰è³‡è¨Šï¼Œæˆ‘å€‘æœƒçµ¦æ‚¨æ›´å¥½çš„å»ºè­°ï¼"

        # âœ… ç‰¹æ®Šè™•ç†ï¼šæ’é™¤èªå¥ + ç‰¹å®šæ–™ç†
        exclusion_phrases = ['ä¸æƒ³åƒ', 'ä¸åƒ', 'ä¸è¦']
        cuisine_phrases = ['ç”œé»', 'æ‹‰éºµ', 'æ—¥å¼', 'éŸ“å¼', 'ä¸­å¼', 'ç¾©å¼', 'ç¾©å¤§åˆ©éºµ', 'ç¾å¼', 'æ¼¢å ¡', 'ç‡’çƒ¤', 'ç«é‹']
        if any(p in user_input for p in exclusion_phrases) and any(c in user_input for c in cuisine_phrases):
            summary.append({"type": "æ’é™¤èªå¥", "message": "å·²æ’é™¤ç‰¹å®šæ–™ç†é¡å‹ï¼Œå¯æ¨è–¦å…¶ä»–é¸é …"})

        # âœ… é€šç”¨èªæ„åˆ†é¡è¦å‰‡
        rules = [
            ("é£²é£Ÿåå¥½", ['ä¸åƒè¾£', 'æ€•è¾£', 'æˆ‘ä¸åƒè¾£'], 'å·²æ’é™¤è¾£å‘³é¸é …ï¼Œæ¨è–¦æ¸…çˆ½ã€æ¹¯å“ç­‰æº«å’Œå£å‘³'),
            ("é£²é£Ÿåå¥½", ['ä¸åƒç‰›', 'æˆ‘ä¸åƒç‰›'], 'å·²æ’é™¤ç‰›è‚‰é¤é»ï¼Œå¯æ¨è–¦é›è‚‰ã€æµ·é®®æˆ–è”¬é£Ÿ'),
            ("é£²é£Ÿåå¥½", ['ä¸åƒæµ·é®®', 'æµ·é®®éæ•'], 'å·²æ’é™¤æµ·é®®é¤å»³ï¼Œæ¨è–¦å…¶ä»–é¡å‹'),
            ("é£²é£Ÿåå¥½", ['åƒç´ ', 'ç´ é£Ÿ', 'æˆ‘åƒç´ '], 'å·²è­˜åˆ¥ç‚ºç´ é£Ÿéœ€æ±‚ï¼Œå¯æ¨è–¦ç´ é£Ÿæˆ–è”¬é£Ÿå‹å–„é¤å»³'),

            ("ç”¨é¤å ´åˆ", ['æœ‹å‹èšé¤', 'åŒå­¸èšé¤', 'èšæœƒ'], 'é©åˆæœ‹å‹èšæœƒï¼Œå¯æ¨è–¦å¹³åƒ¹ç†±é¬§æˆ–å¤šäººå¥—é¤é¤å»³'),
            ("ç”¨é¤å ´åˆ", ['å®¶åº­èšé¤', 'å®¶äººåƒé£¯', 'å®¶æ—èšé¤', 'çˆ¸åª½'], 'é©åˆå®¶åº­ç”¨é¤ï¼Œå»ºè­°é¸æ“‡ç’°å¢ƒå®‰éœã€å¤šæ¨£èœè‰²çš„é¤å»³'),
            ("ç”¨é¤å ´åˆ", ['ç´„æœƒ'], 'æ°£æ°›ä½³çš„æ¨è–¦é©åˆç´„æœƒï¼Œå¯è€ƒæ…®å’–å•¡å»³æˆ–è£æ½¢æº«é¦¨çš„é¤å»³'),
            ("ç”¨é¤å ´åˆ", ['å•†å‹™', 'è«‹å®¢', 'æ­£å¼'], 'æ¨è–¦ç©©é‡æ°£æ°›èˆ‡é«˜è©•åƒ¹çš„é¤å»³ï¼Œé©åˆæ­£å¼æˆ–å•†å‹™ç”¨é€”'),
            ("ç”¨é¤å ´åˆ", ['æ…¶ç”Ÿ', 'ç”Ÿæ—¥', 'æ…¶ç¥'], 'æ¨è–¦æ°£æ°›ä½³ã€æœ‰è›‹ç³•æˆ–åŒ…å»‚çš„é¤å»³ï¼Œé©åˆæ…¶ç¥å ´åˆ'),
            ("ç”¨é¤å ´åˆ", ['å°å­©', 'å°æœ‹å‹', 'å¸¶å­©å­', 'å…’ç«¥'], 'é©åˆè¦ªå­ç”¨é¤ï¼Œå»ºè­°è€ƒæ…®æœ‰å…’ç«¥é¤æˆ–å¯¬æ•ç©ºé–“çš„åº—å®¶'),
            ("ç”¨é¤å ´åˆ", ['é•·è¼©', 'çˆ¶æ¯', 'å®¶äººä¸€èµ·åƒ'], 'å»ºè­°é¸æ“‡ç’°å¢ƒå®‰éœã€é¤é»æ¸…æ·¡çš„å®¶åº­å‹å–„é¤å»³'),

            ("é ç®—", ['ä¸è²´', 'ä¾¿å®œ', 'å¹³åƒ¹', 'åƒ¹æ ¼å¯¦æƒ '], 'åå¥½ä¸è²´çš„é¤å»³ï¼Œå¯ä»¥å„ªå…ˆæŸ¥çœ‹å¹³åƒ¹é«˜è©•åƒ¹é¸é …'),
            ("é ç®—", ['é«˜ç´š', 'é«˜åƒ¹', 'ç²¾è‡´', 'é«˜ç«¯'], 'åå¥½ç²¾ç·»é«”é©—ï¼Œå¯æ¨è–¦é«˜è©•åƒ¹æˆ–é«˜ç«¯é¤å»³'),

            ("æ™‚æ®µ", ['å®µå¤œ', 'æ·±å¤œ'], 'æ¨è–¦å®µå¤œæ™‚æ®µç‡Ÿæ¥­ä¸­çš„è¼•é£Ÿã€ç‚¸ç‰©æˆ–æ‹‰éºµç­‰åº—å®¶'),
            ("æ™‚æ®µ", ['æ—©åˆé¤'], 'å¯æ¨è–¦æ°£æ°›ä½³ã€è©•åƒ¹é«˜çš„æ—©åˆé¤åº—'),
            ("æ™‚æ®µ", ['æ—©é¤'], 'æ¨è–¦ç‡Ÿæ¥­ä¸­çš„ä¸­è¥¿å¼æ—©é¤é¸é …'),

            ("æ–™ç†é¡å‹", ['ç”œé»'], 'æ¨è–¦ç”œé»è©•åƒ¹é«˜çš„é¤å»³æˆ–å’–å•¡å»³'),
            ("æ–™ç†é¡å‹", ['æ‹‰éºµ', 'æ—¥å¼'], 'å¯æ¨è–¦é«˜åˆ†æ—¥å¼é¤å»³èˆ‡æ‹‰éºµååº—'),
            ("æ–™ç†é¡å‹", ['éŸ“å¼'], 'æ¨è–¦é«˜äººæ°£éŸ“å¼æ–™ç†'),
            ("æ–™ç†é¡å‹", ['ä¸­å¼'], 'ä¸­å¼é¤å»³é¸æ“‡è±å¯Œï¼Œæ¨è–¦åˆèœæˆ–ä¾¿ç•¶å‹åº—å®¶'),
            ("æ–™ç†é¡å‹", ['ç¾©å¼', 'ç¾©å¤§åˆ©éºµ'], 'å¯æ¨è–¦ç¾©å¼æ–™ç†èˆ‡ç¾©å¤§åˆ©éºµå°ˆé–€åº—'),
            ("æ–™ç†é¡å‹", ['ç¾å¼', 'æ¼¢å ¡'], 'æ¨è–¦é«˜è©•åƒ¹ç¾å¼æ¼¢å ¡æˆ–ç‚¸ç‰©é¤å»³'),

            ("é£²é£Ÿç‹€æ…‹", ['åƒä¸å¤š', 'åƒå°‘ä¸€é»', 'ç°¡å–®åƒ', 'è¼•é£Ÿ'], 'æ¨è–¦è¼•é£Ÿé¡å‹å¦‚ä¸‰æ˜æ²»ã€æ²™æ‹‰æˆ–æ—©åˆé¤'),
            ("é£²é£Ÿç‹€æ…‹", ['è¶•æ™‚é–“', 'å¿«é€Ÿåƒ', 'æ™‚é–“ä¸å¤š'], 'æ¨è–¦ä¾›é¤å¿«é€Ÿæˆ–å¤–å¸¶æ–¹ä¾¿çš„é¸é …'),
            ("é£²é£Ÿç‹€æ…‹", ['å¤©æ°£å†·', 'æƒ³åƒç†±çš„', 'æš–èƒƒ'], 'æ¨è–¦æ¹¯å“ã€ç«é‹æˆ–ç†±ç‚’ç­‰æº«æš–æ–™ç†'),
            ("é£²é£Ÿç‹€æ…‹", ['æƒ³åƒè¾£', 'é‡å£å‘³', 'è¾£çš„æ–™ç†', 'éº»è¾£', 'è¾£é‹'], 'é©åˆé‡å£å‘³æ„›å¥½è€…ï¼Œæ¨è–¦éº»è¾£ç«é‹ã€å·èœæˆ–éŸ“å¼è¾£ç‚’ç­‰é¤å»³'),
            ("é£²é£Ÿç‹€æ…‹", ['æ¸…æ·¡', 'ä¸æƒ³å¤ªæ²¹', 'åƒæ¸…çˆ½çš„'], 'æ¨è–¦æ¸…çˆ½æˆ–æ¹¯å“é¡å‹ï¼Œé©åˆå£å‘³è¼ƒæ·¡çš„éœ€æ±‚'),
        ]

        for category, keywords, response_text in rules:
            if any(keyword in user_input for keyword in keywords):
                summary.append({"type": category, "message": response_text})

        if not summary:
            summary.append({"type": "å…¶ä»–", "message": default_guidance})

        guidance_combined = "ï¼›".join([item["message"] for item in summary])
        levels = list({item["type"] for item in summary})  # å»é‡é¡åˆ¥

        return Response({
            "status": "success",
            "data": {
                "summary": summary,
                "guidance": guidance_combined,
                "level": levels
            },
            "message": "å·²ç”¢ç”Ÿèªæ„å¼•å°å»ºè­°"
        }, status=status.HTTP_200_OK)



# åŠŸèƒ½ 4ï¼šæ¨è–¦å¡ç‰‡æ¬„ä½æ¨¡æ“¬è¼¸å‡º(å¼·åŒ–ç‰ˆ)
class GenerateCardDataView(APIView):
    permission_classes = [AllowAny]

    def post(self, request):
        # âœ… ä¿éšªè™•ç†ï¼šå…¼å®¹ DRF Request å’Œ WSGIRequestï¼ˆæ•´åˆæ¸¬è©¦ç”¨ï¼‰
        if hasattr(request, 'data'):
            req_type = request.data.get('type')
            restaurants = request.data.get('restaurants', [])
        else:
            req_type = request.POST.get('type')
            try:
                restaurants = json.loads(request.body.decode()).get("restaurants", [])
            except Exception:
                restaurants = []

        if req_type != 'restaurant_list' or not isinstance(restaurants, list):
            return Response({
                "status": "error",
                "data": None,
                "message": "è«‹æä¾› type='restaurant_list' ä¸”åŒ…å« restaurants æ¸…å–®"
            }, status=status.HTTP_400_BAD_REQUEST)

        results = []

        for restaurant in restaurants:
            name = restaurant.get('name', '')
            rating = restaurant.get('rating', 0)
            address = restaurant.get('address', '')
            price_level = restaurant.get('price_level', '')
            review_count = restaurant.get('review_count', 0)
            is_open_raw = restaurant.get('is_open', None)
            matched_tags = restaurant.get('matched_tags', [])
            ai_reason = restaurant.get('ai_reason', '')
            highlight = restaurant.get('highlight', '')
            distance_m = restaurant.get('distance_m', random.randint(100, 2000))
            distance = f"{distance_m} å…¬å°º"

            # å…±ç”¨è™•ç†é‚è¼¯
            map_url = generate_map_url(name)
            is_open = format_open_status(is_open_raw)
            district = extract_district(address)
            price_desc = generate_price_description(price_level)

            # çµ„åˆ tags
            tags = list(set(matched_tags + ([district] if district else []) + ([price_desc] if price_desc else [])))

            # highlight è£œå¼·
            if not highlight:
                if "ç”œé»" in tags or "è›‹ç³•" in name:
                    highlight = "ç”œé»è©•åƒ¹é«˜"
                elif rating >= 4.5:
                    highlight = "è©•åƒ¹å„ªè‰¯"
                elif district and name not in ["æ³°å¼å°é¤¨"]:
                    highlight = "åœ°é»ä¾¿åˆ©"
                else:
                    highlight = ""

            # æ¨è–¦ç†ç”±
            recommend_reason = generate_recommend_reason(matched_tags, highlight, district, price_desc)

            # æ¨¡æ“¬ featuresï¼ˆé‚è¼¯æ“´å……ï¼‰
            features = []
            if "ç´ é£Ÿ" in tags:
                features.append("æä¾›ç´ é£Ÿ")
            if price_desc == "åƒ¹æ ¼å¯¦æƒ ":
                features.append("é«˜ CP å€¼")
            if "ç”œé»" in tags or "è›‹ç³•" in name:
                features.append("ç”œé»å°ˆé–€")
            if rating >= 4.5 and review_count >= 300:
                features.append("äººæ°£é¤å»³")
            if price_level == "$":
                features.append("åƒ¹æ ¼ä¾¿å®œ")
            if "ç•°åœ‹æ–™ç†" in tags or "æ³°å¼" in name:
                features.append("ç•°åœ‹æ–™ç†")

            # æ¨¡æ“¬ styleï¼ˆå…ˆè™•ç†å¤œè²“ï¼Œå†çœ‹å…¶ä»–ï¼‰
            style = ""
            if "æ³°å¼" in name or "æ±å—äº" in tags:
                style = "æ±å—äºé¢¨"
            elif "å¤œè²“æ—" in tags or "å¤œè²“" in name or "å®µå¤œ" in tags or distance_m > 1500:
                style = "å¤œè²“æ—"
            elif "æ–‡é’" in name or "å’–å•¡" in name or "ç”œé»" in tags:
                style = "æ–‡é’"
            elif "ç‡’è‚‰" in name or "çƒ¤è‚‰" in tags:
                style = "ç¾å¼"
            elif "å£½å¸" in name or "æ—¥å¼" in tags or "æ‹‰éºµ" in name:
                style = "æ—¥å¼"

            # æ¨¡æ“¬ç‡Ÿæ¥­æ™‚é–“èˆ‡é ç•™æ¬„ä½
            opening_hours = "11:00 - 21:00"
            has_coupon = False
            image_url = ""

            results.append({
                "name": name,
                "rating": rating,
                "address": address,
                "tags": tags,
                "highlight": highlight,
                "distance": distance,
                "distance_m": distance_m,
                "review_count": review_count,
                "price_level": price_level,
                "is_open": is_open,
                "map_url": map_url,
                "features": features,
                "style": style,
                "opening_hours": opening_hours,
                "recommend_reason": recommend_reason,
                "has_coupon": has_coupon,
                "image_url": image_url
            })

        return Response({
            "status": "success",
            "data": {
                "results": results
            },
            "message": "å¡ç‰‡æ¬„ä½è³‡æ–™å·²ç”¢ç”Ÿ"
        }, status=status.HTTP_200_OK)


# âœ… æ•´åˆæ¸¬è©¦ï¼šåŠŸèƒ½ä¸€ â†’ å›› â†’ äºŒï¼ˆæœ€çµ‚å¼·åŒ–ç‰ˆï¼Œä¸€æ¬¡åˆ°ä½ï¼‰
class IntegrationTestView(APIView):
    permission_classes = [AllowAny]
    parser_classes = [JSONParser]  # âœ… æ”¯æ´ application/json

    def post(self, request):
        from .sample_data import RESTAURANTS_SAMPLE
        from .views import ExtractNegativeConditionsView, GenerateCardDataView, GenerateRecommendReasonView

        factory = APIRequestFactory()

        input_text = request.data.get("text", "").strip()

        # âœ… å®‰å…¨å¸ƒæ—è§£æï¼šé¿å… "false" ä»è¢«ç•¶æˆ True
        raw_allow = request.data.get("allow_backfill", True)
        if isinstance(raw_allow, str):
            allow_backfill = raw_allow.strip().lower() in ("1", "true", "t", "yes", "y", "on")
        else:
            allow_backfill = bool(raw_allow)

        if not input_text:
            return Response({
                "status": "error",
                "data": None,
                "message": "è«‹æä¾› text æ¬„ä½"
            }, status=status.HTTP_400_BAD_REQUEST)

        print("\nğŸ¯ æ•´åˆæ¸¬è©¦é–‹å§‹ >>>")
        print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{input_text}")
        print(f"ğŸ§© allow_backfill = {allow_backfill}")

        # =========================
        # âœ… Step 1ï¼šåŠŸèƒ½ä¸€ï¼ˆæ’é™¤æ¢ä»¶æ“·å–ï¼‰
        request_exclusion = factory.post("/fake_path/", {"text": input_text}, format='json')
        wrapped_request = Request(request_exclusion, parsers=[JSONParser()])
        exclusion_response = ExtractNegativeConditionsView().post(wrapped_request)

        if hasattr(exclusion_response, "data") and isinstance(exclusion_response.data, dict):
            exclusion_data_raw = exclusion_response.data
        elif hasattr(exclusion_response, "_data") and isinstance(exclusion_response._data, dict):
            exclusion_data_raw = exclusion_response._data
        else:
            exclusion_data_raw = {}

        excluded_items = []
        if isinstance(exclusion_data_raw, dict):
            data_field = exclusion_data_raw.get("data", {})
            if isinstance(data_field, dict):
                excluded_items = data_field.get("excluded", [])
        print(f"ğŸš« ä¸Šæ¸¸æ’é™¤é …ç›®ï¼š{excluded_items}")

        # =========================
        # âœ… Step 2ï¼šé€²éšæ¯”å°æ’é™¤ï¼ˆæ¸…ç†æ¨™é»/å…¨å½¢ç©ºç™½ + åŒç¾©è© + åˆä½µæ¬„ä½å­—ä¸² + äºŒæ¬¡ä¿åº• + å¯æ§è£œä½ï¼‰
        import re

        def _normalize(s: str) -> str:
            if not isinstance(s, str):
                return ""
            s = s.strip().lower()
            # ç§»é™¤å¸¸è¦‹ä¸­è‹±æ¨™é»ã€å…¨å½¢ç©ºç™½èˆ‡ç¬¦è™Ÿ
            s = re.sub(r"[ï¼Œ,ã€‚.!ï¼Ÿ?ã€/\\|()\[\]ã€ã€‘{}\-ï¼¿_~^'\"`ï¼š:ï¼›;@#*$ï¼‹+ï¼=ï¼Â·ï½¥\s]+", "", s)
            return s

        def _expand_exclusions(items: list) -> list:
            mapping = {
                "ç”œé»": ["ç”œé»", "ç”œå“", "ç”œé£Ÿ", "è›‹ç³•", "çƒ˜ç„™", "ä¸‹åˆèŒ¶", "ç”œé»åº—", "ç”œé»å°ˆé–€", "ç”œé»è©•åƒ¹é«˜"],
                "æ‹‰éºµ": ["æ‹‰éºµ", "ramen"],
                "ç‡’çƒ¤": ["ç‡’çƒ¤", "çƒ¤è‚‰", "ç‚­ç«", "ç‡’è‚‰"],
                "æ¼¢å ¡": ["æ¼¢å ¡", "burger"],
                "ç¾å¼": ["ç¾å¼", "ç¾å¼é¤å»³", "ç¾å¼é¢¨æ ¼", "ç¾å¼æ¼¢å ¡"],
                "ç«é‹": ["ç«é‹", "é‹ç‰©", "æ¶®æ¶®é‹", "éº»è¾£é‹"],
            }
            expanded = set()
            for it in items or []:
                raw = (it or "").strip()
                norm = _normalize(raw)
                if not norm:
                    continue
                # åŸå­—èˆ‡æ­£è¦åŒ–å­—éƒ½æ‹¿å» mapping æ‰¾åŒç¾©è©
                candidates = set([raw, norm])
                for key, syns in mapping.items():
                    if key == raw or key == norm:
                        candidates |= set(syns)
                # æ‰€æœ‰å€™é¸è©æ­£è¦åŒ–å¾ŒåŠ å…¥
                for c in candidates:
                    expanded.add(_normalize(c))
            return list(expanded)

        def _collect_blob(r: dict) -> str:
            # æŠŠå¯èƒ½åŒ…å«å“é¡è¨Šæ¯çš„æ¬„ä½åˆä½µï¼Œæå‡å‘½ä¸­ç‡
            parts = []
            for k in ["name", "highlight", "ai_reason", "comment_summary", "style", "address"]:
                v = r.get(k, "")
                if v:
                    parts.append(str(v))
            for lk in ["matched_tags", "tags", "features"]:
                seq = r.get(lk) or []
                parts.extend([str(t) for t in seq])
            return _normalize("ï½œ".join(parts))

        expanded_ex = set(_expand_exclusions(excluded_items))
        print(f"ğŸ§© æ“´å……å¾Œæ’é™¤è©ï¼ˆnormï¼‰ï¼š{sorted(expanded_ex)}")

        # çµ±ä¸€å…ˆç”Ÿæˆæ¯é–“é¤å»³çš„ã€Œæ­£è¦åŒ–åˆä½µå­—ä¸²ã€
        normalized_blobs = [(_collect_blob(r), r) for r in RESTAURANTS_SAMPLE]

        filtered = []
        debug_hits = []
        for blob, r in normalized_blobs:
            hit = None
            for ex in expanded_ex:
                if ex and ex in blob:
                    hit = ex
                    break
            if hit:
                debug_hits.append((r.get("name", ""), hit))
            else:
                filtered.append(r)

        # ğŸ›¡ï¸ äºŒæ¬¡ä¿åº•ï¼šè‹¥å®Œå…¨æ²’å‰”é™¤ï¼Œç”¨ã€Œåç¨±åŒ…å«åŸå§‹æ’é™¤è©ã€å†éä¸€è¼ª
        if len(filtered) == len(RESTAURANTS_SAMPLE):
            _raw_ex = [(it or "").strip() for it in (excluded_items or []) if (it or "").strip()]
            if _raw_ex:
                tmp = []
                for r in filtered:
                    name = r.get("name", "")
                    if any(x in name for x in _raw_ex):
                        debug_hits.append((name, f"fallback:{'/'.join(_raw_ex)}"))
                    else:
                        tmp.append(r)
                filtered = tmp

        print(f"âœ… é€šéæ’é™¤ç¯©é¸çš„é¤å»³æ•¸ï¼š{len(filtered)}")
        print("ğŸ” è¢«æ’é™¤æ¸…å–®/å‘½ä¸­è©ï¼š", debug_hits)

        # â€”â€” è£œä½æ©Ÿåˆ¶ï¼ˆå¯æ§ï¼‰â€”â€”
        TARGET_N = 5
        before_fill = len(filtered)
        used_backfill_pool = False

        if allow_backfill and len(filtered) < TARGET_N:
            picked_names = set(r.get("name", "") for r in filtered)
            candidate_pool = []
            for r in RESTAURANTS_SAMPLE:
                nm = r.get("name", "")
                if nm in picked_names:
                    continue
                blob = _collect_blob(r)
                if any(ex in blob for ex in expanded_ex):
                    continue
                candidate_pool.append(r)

            # ç”¨ ratingã€review_count åšæ’åºè£œä½
            candidate_pool.sort(
                key=lambda x: (float(x.get("rating") or 0), int(x.get("review_count") or 0)),
                reverse=True
            )
            for r in candidate_pool:
                if len(filtered) >= TARGET_N:
                    break
                nm = r.get("name", "")
                if nm not in picked_names:
                    filtered.append(r)
                    picked_names.add(nm)

        # å¯é¸ï¼šå¾Œå‚™åå–®ï¼ˆè‹¥ä»ä¸è¶³ 5ï¼‰
        if allow_backfill and len(filtered) < TARGET_N:
            BACKFILL_POOL = [
                {
                    "name": "é’è”¬ä¾¿ç•¶èˆ–",
                    "address": "å°åŒ—å¸‚ä¸­æ­£å€",
                    "rating": 4.3, "review_count": 210,
                    "price_level": "$", "style": "å®¶å¸¸",
                    "features": ["åƒ¹æ ¼å¯¦æƒ ", "å¿«é€Ÿæ–¹ä¾¿"],
                    "matched_tags": ["ç´ é£Ÿéœ€æ±‚", "æ¸…çˆ½å£å‘³"],
                    "highlight": "å¹³åƒ¹æ¸…çˆ½",
                    "ai_reason": "å¹³åƒ¹æ¸…çˆ½ã€ä¾¿ç•¶é¸æ“‡å¤šï¼Œé©åˆå¿«é€Ÿç”¨é¤",
                },
                {
                    "name": "å°å··æ¸…ç²¥åº—",
                    "address": "å°åŒ—å¸‚å¤§åŒå€",
                    "rating": 4.2, "review_count": 180,
                    "price_level": "$", "style": "å‚³çµ±",
                    "features": ["æ¸…çˆ½å£å‘³", "è¦ªå­å‹å–„"],
                    "matched_tags": ["æ¸…çˆ½å£å‘³"],
                    "highlight": "æ¸…ç²¥å°èœ",
                    "ai_reason": "å£å‘³æ¸…çˆ½ã€é©åˆå¸¶é•·è¼©ï¼Œé¤é»é¸æ“‡å¤š",
                },
                {
                    "name": "å—æ´‹ç±³ç·šé¤¨",
                    "address": "å°åŒ—å¸‚è¬è¯å€",
                    "rating": 4.4, "review_count": 260,
                    "price_level": "$$", "style": "æ±å—äºé¢¨",
                    "features": ["åƒ¹æ ¼å¯¦æƒ ", "é«˜ CP å€¼"],
                    "matched_tags": ["æ±å—äºé¢¨æ ¼", "åƒ¹æ ¼å¯¦æƒ "],
                    "highlight": "ç•°åœ‹é¢¨å‘³",
                    "ai_reason": "æ±å—äºé¢¨å‘³ã€ä»½é‡å……è¶³ã€åƒ¹æ ¼å¯¦æƒ ",
                },
            ]
            picked_names = set(r.get("name", "") for r in filtered)
            safe_backfills = []
            for r in BACKFILL_POOL:
                if r["name"] in picked_names:
                    continue
                blob = _collect_blob(r)
                if any(ex in blob for ex in expanded_ex):
                    continue
                safe_backfills.append(r)

            safe_backfills.sort(
                key=lambda x: (float(x.get("rating") or 0), int(x.get("review_count") or 0)),
                reverse=True
            )
            for r in safe_backfills:
                if len(filtered) >= TARGET_N:
                    break
                filtered.append(r)
                picked_names.add(r.get("name", ""))
                used_backfill_pool = True

        if allow_backfill:
            print(
                f"ğŸ“Œ è£œä½å¾Œçš„é¤å»³æ•¸ï¼š{len(filtered)}ï¼ˆç›®æ¨™ {TARGET_N}ï¼‰ï¼›"
                f"ä¾†æºï¼š{'BACKFILL_POOL' if used_backfill_pool else ('RESTAURANTS_SAMPLE' if len(filtered) > before_fill else 'ç„¡')}"
            )

        # =========================
        # âœ… Step 3ï¼šåŠŸèƒ½å››ï¼ˆæ¬„ä½è£œå¼·ï¼‰
        request_card_data = factory.post("/fake_path/", {
            "type": "restaurant_list",
            "restaurants": filtered
        }, format='json')
        wrapped_card_request = Request(request_card_data, parsers=[JSONParser()])
        card_data_response = GenerateCardDataView().post(wrapped_card_request)

        if hasattr(card_data_response, "data") and isinstance(card_data_response.data, dict):
            card_data_raw = card_data_response.data
        elif hasattr(card_data_response, "_data") and isinstance(card_data_response._data, dict):
            card_data_raw = card_data_response._data
        else:
            card_data_raw = {}

        card_restaurants = card_data_raw.get("data", {}).get("results", [])
        print(f"ğŸ“¦ è£œå®Œæ¬„ä½çš„é¤å»³æ•¸ï¼š{len(card_restaurants)}")

        # =========================
        # âœ… Step 4ï¼šåŠŸèƒ½äºŒï¼ˆæ¨è–¦ç†ç”±è£œå¼· + ä¿åº•æ’é™¤ï¼‰
        request_reason = factory.post("/fake_path/", {
            "type": "restaurant_list",
            "restaurants": card_restaurants,
            "user_input": input_text,
            "excluded_items": excluded_items,   # â­ é—œéµï¼šæŠŠä¸Šæ¸¸æ’é™¤è©å‚³ä¸‹å»
        }, format='json')
        wrapped_reason_request = Request(request_reason, parsers=[JSONParser()])
        final_response = GenerateRecommendReasonView().post(wrapped_reason_request)

        if hasattr(final_response, "data") and isinstance(final_response.data, dict):
            final_data_raw = final_response.data
        elif hasattr(final_response, "_data") and isinstance(final_response._data, dict):
            final_data_raw = final_response._data
        else:
            final_data_raw = {}

        # æ­£ç¢ºè¨ˆç®—æœ€çµ‚ç­†æ•¸ï¼ˆresults æ˜¯ listï¼‰
        results_list = final_data_raw.get("data", {}).get("results", [])
        print(f"ğŸŒŸ æœ€çµ‚æ¨è–¦çµæœç­†æ•¸ï¼š{len(results_list) if isinstance(results_list, list) else 0}")

        # âœ… å›å‚³çµæ§‹ï¼šç¶­æŒä½ åŸæœ¬é¢¨æ ¼ï¼Œå¤–åŠ æŠŠ excluded_items ä¸€èµ·å›å‚³ï¼Œæ–¹ä¾¿æ¸¬è©¦å°å‡ºæˆ–é©—è­‰
        return Response({
            "status": "success",
            "data": final_data_raw.get("data", {}),
            "excluded_items": excluded_items,
            "message": "æ•´åˆæµç¨‹å·²åŸ·è¡Œå®Œæˆ"
        }, status=status.HTTP_200_OK)




